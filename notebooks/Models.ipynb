{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloading import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA_TRAIN         = '../data/preprocessed-v2/train_vectors_scaled.csv'\n",
    "PATH_DATA_VALIDATION    = '../data/preprocessed-v2/validation_vectors_scaled.csv'\n",
    "PATH_DATA_TEST          = '../data/preprocessed-v2/test_vectors_scaled.csv'\n",
    "TEST_SIZE = .2\n",
    "UNIQUE_COLUMNS = False\n",
    "\n",
    "(training_data_df, training_target_df) = get_all_data(\n",
    "    path_all_vectors=PATH_DATA_TRAIN, unique=UNIQUE_COLUMNS, remove_columns=['is_synthetic'])\n",
    "\n",
    "(validation_data_df, validation_target_df) = get_all_data(\n",
    "    path_all_vectors=PATH_DATA_VALIDATION, unique=UNIQUE_COLUMNS)\n",
    "\n",
    "(test_data_df, test_target_df) = get_all_data(\n",
    "    path_all_vectors=PATH_DATA_TEST, unique=UNIQUE_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert inputs are of same length\n",
    "assert training_data_df.columns.to_list() == validation_data_df.columns.to_list() == test_data_df.columns.to_list()\n",
    "assert training_target_df.columns.to_list() == validation_target_df.columns.to_list() == test_target_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Avg packet len', 'CHARGEN', 'CLDAP', 'CoAP', 'DNS', 'Data speed', 'Generic UDP', 'High volume traffic', 'IPv4 fragmentation', 'NTP', 'Packet speed', 'Port number', 'RDP', 'SNMP', 'SSDP', 'SYN Attack', 'Significant flag', 'Source IP count', 'Suspicious traffic', 'TCP Anomaly', 'is_weekday', 'other_attack_codes', 'time_of_day', 'victim IP num']\n",
      "['Avg packet len', 'CHARGEN', 'CLDAP', 'CoAP', 'DNS', 'Data speed', 'Generic UDP', 'High volume traffic', 'IPv4 fragmentation', 'NTP', 'Packet speed', 'Port number', 'RDP', 'SNMP', 'SSDP', 'SYN Attack', 'Significant flag', 'Source IP count', 'Suspicious traffic', 'TCP Anomaly', 'is_weekday', 'other_attack_codes', 'time_of_day', 'victim IP num']\n",
      "['Avg packet len', 'CHARGEN', 'CLDAP', 'CoAP', 'DNS', 'Data speed', 'Generic UDP', 'High volume traffic', 'IPv4 fragmentation', 'NTP', 'Packet speed', 'Port number', 'RDP', 'SNMP', 'SSDP', 'SYN Attack', 'Significant flag', 'Source IP count', 'Suspicious traffic', 'TCP Anomaly', 'is_weekday', 'other_attack_codes', 'time_of_day', 'victim IP num']\n"
     ]
    }
   ],
   "source": [
    "print((training_data_df.columns.to_list()))\n",
    "print((validation_data_df.columns.to_list()))\n",
    "print((test_data_df.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data_df.to_numpy()\n",
    "X_validation = validation_data_df.to_numpy()\n",
    "X_test = test_data_df.to_numpy()\n",
    "\n",
    "y_train = training_target_df.to_numpy().squeeze(1)\n",
    "y_validation = validation_target_df.to_numpy().squeeze(1)\n",
    "y_test = test_target_df.to_numpy().squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (1346829,)\n",
      "y_validation.shape:  (1247266,)\n",
      "y_test.shape:  (1233449,)\n"
     ]
    }
   ],
   "source": [
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_validation.shape: ', y_validation.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# MODEL INIT\n",
    "model = RandomForestClassifier(\n",
    "    verbose=True,\n",
    "    max_depth=20,\n",
    "    max_features=0.7,\n",
    "    n_estimators=100,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:  1.9min\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "\n",
    "_ = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 trees have length 20\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "count = defaultdict(int)\n",
    "for tree in model.estimators_:\n",
    "    # print(tree.get_depth(), end=', ')\n",
    "    count[tree.get_depth()] += 1\n",
    "count = dict(count)\n",
    "for item, key in count.items():\n",
    "    print(f'{key} trees have length {item}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.008017535954639988 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# get predictions\n",
    "y_pred = model.predict(X_validation)\n",
    "\n",
    "matches = np.count_nonzero(y_validation == y_pred)\n",
    "print(f'Accuracy: {100 * matches / len(y_validation)} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------+\n",
      "|       Property      | Importance (%) |\n",
      "+---------------------+----------------+\n",
      "|    victim IP num    |      35 %      |\n",
      "|    Avg packet len   |      15 %      |\n",
      "|   Source IP count   |      10 %      |\n",
      "|     time_of_day     |      8 %       |\n",
      "|     Port number     |      8 %       |\n",
      "|      Data speed     |      6 %       |\n",
      "|  Suspicious traffic |      4 %       |\n",
      "| High volume traffic |      3 %       |\n",
      "|   Significant flag  |      3 %       |\n",
      "|     Packet speed    |      1 %       |\n",
      "|      is_weekday     |      <1 %      |\n",
      "|        CLDAP        |      <1 %      |\n",
      "|         DNS         |      <1 %      |\n",
      "|      SYN Attack     |      <1 %      |\n",
      "|     Generic UDP     |      <1 %      |\n",
      "|         NTP         |      <1 %      |\n",
      "|  IPv4 fragmentation |      <1 %      |\n",
      "|         SNMP        |      <1 %      |\n",
      "|         CoAP        |      <1 %      |\n",
      "|         SSDP        |      <1 %      |\n",
      "|     TCP Anomaly     |      <1 %      |\n",
      "|       CHARGEN       |      <1 %      |\n",
      "|  other_attack_codes |      <1 %      |\n",
      "|         RDP         |      0 %       |\n",
      "+---------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "info_data = list(training_data_df.columns.values)\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Property\", \"Importance (%)\"]\n",
    "\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices:\n",
    "    imp = 100 * importances[i]\n",
    "\n",
    "    if imp > 0:\n",
    "        if int(imp) == 0:\n",
    "            imp = '<1 %'\n",
    "        else:\n",
    "            imp = f'{int(imp)} %'\n",
    "    else:\n",
    "        imp = '0 %'\n",
    "\n",
    "\n",
    "    table.add_row([info_data[i], imp])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WanDB init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "WanDB not yet needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWanDB not yet needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: WanDB not yet needed."
     ]
    }
   ],
   "source": [
    "assert False, \"WanDB not yet needed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# start a new wandb run and add your model hyperparameters\n",
    "wandb.init(project='Halado_Adatelemzes_Labor', config=model.get_params())\n",
    "\n",
    "# Add additional configs to wandb\n",
    "wandb.config.update({\"test_size\" : TEST_SIZE,\n",
    "                    \"train_len\" : len(X_train),\n",
    "                    \"test_len\" : len(X_validation)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.sklearn import plot_precision_recall, plot_feature_importances\n",
    "from wandb.sklearn import plot_class_proportions, plot_learning_curve, plot_roc\n",
    "\n",
    "y_probas = model.predict_proba(X_validation)\n",
    "\n",
    "# log additional visualisations to wandb\n",
    "plot_class_proportions(y_train, y_validation, info_data)\n",
    "# plot_learning_curve(model, X_train, y_train)\n",
    "plot_roc(y_validation, y_probas, info_data)\n",
    "plot_precision_recall(y_validation, y_probas, info_data)\n",
    "plot_feature_importances(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
