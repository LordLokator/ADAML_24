{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloading import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +---------------------+----------------+\n",
    "# |       Property      | Importance (%) |\n",
    "# +---------------------+----------------+\n",
    "# |......................................|\n",
    "# |        CLDAP        |      <1 %      |\n",
    "# |      is_weekday     |      <1 %      |\n",
    "# |         DNS         |      <1 %      |\n",
    "# |      SYN Attack     |      <1 %      |\n",
    "# |     Generic UDP     |      <1 %      |\n",
    "# |         NTP         |      <1 %      |\n",
    "# |  IPv4 fragmentation |      <1 %      |\n",
    "# |         CoAP        |      <1 %      |\n",
    "# |         SNMP        |      <1 %      |\n",
    "# |         SSDP        |      <1 %      |\n",
    "# |     TCP Anomaly     |      <1 %      |\n",
    "# |       CHARGEN       |      <1 %      |\n",
    "# |  other_attack_codes |      <1 %      |\n",
    "# |         RDP         |      0 %       |\n",
    "# +---------------------+----------------+\n",
    "\n",
    "irrelevants = [\n",
    "    'CLDAP',\n",
    "    'is_weekday',\n",
    "    'DNS',\n",
    "    'SYN Attack',\n",
    "    'Generic UDP',\n",
    "    'NTP',\n",
    "    'IPv4 fragmentation',\n",
    "    'CoAP',\n",
    "    'SNMP',\n",
    "    'SSDP',\n",
    "    'TCP Anomaly',\n",
    "    'CHARGEN',\n",
    "    'other_attack_codes',\n",
    "    'RDP'\n",
    "]\n",
    "irrelevants = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA_TRAIN         = '../data/preprocessed-v2/train_vectors_scaled.csv'\n",
    "PATH_DATA_VALIDATION    = '../data/preprocessed-v2/validation_vectors_scaled.csv'\n",
    "PATH_DATA_TEST          = '../data/preprocessed-v2/test_vectors_scaled.csv'\n",
    "PATH_DATA_GEN          = '../data/preprocessed-v2/generalisation_vectors_scaled.csv'\n",
    "TEST_SIZE = .2\n",
    "UNIQUE_COLUMNS = False\n",
    "\n",
    "(training_data_df, training_target_df) = get_all_data(\n",
    "    path_all_vectors=PATH_DATA_TRAIN, unique=UNIQUE_COLUMNS, remove_columns=['is_synthetic'] + irrelevants)\n",
    "\n",
    "(validation_data_df, validation_target_df) = get_all_data(\n",
    "    path_all_vectors=PATH_DATA_VALIDATION, unique=UNIQUE_COLUMNS, remove_columns=irrelevants)\n",
    "\n",
    "(test_data_df, test_target_df) = get_all_data(\n",
    "    path_all_vectors=PATH_DATA_TEST, unique=UNIQUE_COLUMNS, remove_columns=irrelevants)\n",
    "\n",
    "(gen_data_df, gen_target_df) = get_all_data(\n",
    "    path_all_vectors=PATH_DATA_GEN, unique=UNIQUE_COLUMNS, remove_columns=irrelevants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert inputs are of same length\n",
    "assert training_data_df.columns.to_list() == validation_data_df.columns.to_list() == test_data_df.columns.to_list() == gen_data_df.columns.to_list()\n",
    "assert training_target_df.columns.to_list() == validation_target_df.columns.to_list() == test_target_df.columns.to_list() == gen_target_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((training_data_df.columns.to_list()))\n",
    "print((validation_data_df.columns.to_list()))\n",
    "print((test_data_df.columns.to_list()))\n",
    "print((gen_data_df.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data_df.to_numpy()\n",
    "X_validation = validation_data_df.to_numpy()\n",
    "X_test = test_data_df.to_numpy()\n",
    "X_gen = gen_data_df.to_numpy()\n",
    "\n",
    "y_train = training_target_df.to_numpy().squeeze(1)\n",
    "y_validation = validation_target_df.to_numpy().squeeze(1)\n",
    "y_test = test_target_df.to_numpy().squeeze(1)\n",
    "y_gen = gen_target_df.to_numpy().squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train.shape: ', X_train.shape)\n",
    "print('X_validation.shape: ', X_validation.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('X_gen.shape: ', X_gen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_validation.shape: ', y_validation.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "print('y_gen.shape: ', y_gen.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# MODEL INIT\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    max_features=0.7,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    verbose=True,\n",
    "    )\n",
    "\n",
    "# model = GradientBoostingClassifier(\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=1.0,\n",
    "#     max_depth=1,\n",
    "#     random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "_ = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# count = defaultdict(int)\n",
    "# for tree in model.estimators_:\n",
    "#     print(tree.get_depth(), end=', ')\n",
    "#     count[tree.get_depth()] += 1\n",
    "# count = dict(count)\n",
    "# for item, key in count.items():\n",
    "#     print(f'{key} trees have length {item}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test(dataset:np.ndarray, target:np.ndarray):\n",
    "    # get predictions\n",
    "    y_pred = model.predict(dataset)\n",
    "\n",
    "    matches = np.count_nonzero(target == y_pred)\n",
    "    print(f'Accuracy: {100 * matches / len(target)} %')\n",
    "\n",
    "\n",
    "test(X_train, y_train)\n",
    "test(X_validation, y_validation)\n",
    "test(X_test, y_test)\n",
    "test(X_gen, y_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"random_forest_85%gen.joblib\")\n",
    "# model = joblib.load(\"random_forest.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "info_data = list(training_data_df.columns.values)\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Property\", \"Importance (%)\"]\n",
    "\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices:\n",
    "    imp = 100 * importances[i]\n",
    "\n",
    "    if imp > 0:\n",
    "        if int(imp) == 0:\n",
    "            imp = '<1 %'\n",
    "        else:\n",
    "            imp = f'{int(imp)} %'\n",
    "    else:\n",
    "        imp = '0 %'\n",
    "\n",
    "\n",
    "    table.add_row([info_data[i], imp])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WanDB init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"WanDB not yet needed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# start a new wandb run and add your model hyperparameters\n",
    "wandb.init(project='Halado_Adatelemzes_Labor', config=model.get_params())\n",
    "\n",
    "# Add additional configs to wandb\n",
    "wandb.config.update({\"test_size\" : TEST_SIZE,\n",
    "                    \"train_len\" : len(X_train),\n",
    "                    \"test_len\" : len(X_validation)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.sklearn import plot_precision_recall, plot_feature_importances\n",
    "from wandb.sklearn import plot_class_proportions, plot_learning_curve, plot_roc\n",
    "\n",
    "y_probas = model.predict_proba(X_validation)\n",
    "\n",
    "# log additional visualisations to wandb\n",
    "plot_class_proportions(y_train, y_validation, info_data)\n",
    "# plot_learning_curve(model, X_train, y_train)\n",
    "plot_roc(y_validation, y_probas, info_data)\n",
    "plot_precision_recall(y_validation, y_probas, info_data)\n",
    "plot_feature_importances(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
